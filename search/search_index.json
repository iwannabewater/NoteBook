{"config":{"lang":["en","ja"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>Welcome to my Docs!</p>"},{"location":"ai/cv/onnx_deployment/","title":"\u6a21\u578b\u7ec8\u7aef\u90e8\u7f72","text":""},{"location":"ai/cv/onnx_deployment/#1","title":"1. \u5f00\u53d1\u90e8\u7f72\u6d41\u7a0b\uff1a","text":""},{"location":"ai/cv/onnx_deployment/#2-pytorch","title":"2. PyTorch\u6a21\u578b\u90e8\u7f72\u901a\u7528\u6d41\u7a0b\uff1a","text":""},{"location":"ai/cv/onnx_deployment/#3","title":"3. \u73af\u5883\u914d\u7f6e\uff1a","text":""},{"location":"ai/cv/onnx_deployment/#31","title":"3.1 \u672c\u5730\u673a\u5668\uff1a","text":"<pre><code># \u5b89\u88c5pytorch\npip install torch torchvision torchaudio\n\n# \u5b89\u88c5onnx\npip install onnx -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n# \u5b89\u88c5\u63a8\u7406\u5f15\u64ceonnx RT\npip install onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n# \u5176\u4ed6\u53ef\u7528\u4f9d\u8d56\npip install numpy pandas matplotlib opencv-python pillow -i https://pypi.tuna.tsinghua.edu.cn/simple\n</code></pre>"},{"location":"ai/cv/onnx_deployment/#32-arm-","title":"3.2 ARM\u673a\u5668 -- \u677e\u7075\u5c0f\u8f66\uff1a","text":"<pre><code># \u5b89\u88c5\u57fa\u7840\u5de5\u5177\u5305\npip install numpy pandas matplotlib opencv-python pillow onnx onnxruntime -i https://pypi.tuna.tsinghua.edu.cn/simple\n\n# \u5b89\u88c5pytorch (\u4e0d\u8981\u6709\u5176\u4ed6\u989d\u5916\u64cd\u4f5c\uff0c\u4f1a\u81ea\u52a8\u5339\u914d\u4e0b\u8f7darm-torch)\npip install torch torchvision torchaudio\n</code></pre>"},{"location":"ai/cv/onnx_deployment/#4-pthonnx","title":"4. pth\u683c\u5f0f\u8f6connx\u683c\u5f0f\uff1a","text":"<p>eg: torch2onnx.py</p> <p>(\u8fd9\u4e00\u6b65\u5728\u672c\u5730\u673a\u5668\u4e0a\u6267\u884c\uff0c\u4e4b\u540e\u5bfc\u51faonnx\u6a21\u578b\u4e4b\u540escp\u6216\u8005ftp\u5230\u677e\u6797\u5c0f\u8f66\u4e0a)</p> <pre><code>import torch\nimport torch.nn as nn\nimport onnx\n\n# here is your Net\nclass Alexnet(nn.Module):\n    def __init__(self, output_dim):\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=64,\n                      kernel_size=3, stride=2, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=64, out_channels=192,\n                      kernel_size=3, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=192, out_channels=384,\n                      kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=384, out_channels=256,\n                      kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(in_channels=256, out_channels=256,\n                      kernel_size=3, padding=1),\n            nn.MaxPool2d(kernel_size=2),\n            nn.ReLU(inplace=True),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(256*7*7, 1000),\n            nn.ReLU(inplace=True),\n\n            nn.Dropout(0.5),\n            nn.Linear(in_features=1000, out_features=256),\n            nn.ReLU(inplace=True),\n\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        h = x.view(x.shape[0], -1)\n        x = self.classifier(h)\n        return x\n\n# 43 classes\nnumClasses = 43\n\n# \u52a0\u8f7dpth\u6a21\u578b\nmodel = Alexnet(numClasses)\nmodel.load_state_dict(torch.load('../checkpoints/tsr_alexnet_30epochs.pth'))\n\n# eval mode\nmodel.eval() \n\n# \u6784\u9020\u56fe\u50cf\u8f93\u5165tensor\nexample_input = torch.randn(1, 3, 112, 112)\n\n# output onnx\noutput_onnx_file = '../checkpoints/tsr_alexnet_30epochs_v2.onnx'\ntorch.onnx.export(model,\n                  example_input,\n                  output_onnx_file,\n                  export_params=True,\n                  opset_version=13,  # \u53ef\u9009\u7248\u672c\n                  do_constant_folding=True,  # \u4f18\u5316\u6a21\u578b\n                  input_names=['input'],\n                  output_names=['output'])\n\n# \u9a8c\u8bc1\u6a21\u578b\nonnx_model = onnx.load(output_onnx_file)\nonnx.checker.check_model(onnx_model)\n\nprint(\"ONNX output successful!\")\n</code></pre> <p>ps: \u5f97\u5230onnx\u683c\u5f0f\u7684\u6a21\u578b\u4e4b\u540e\u5373\u53ef\u90e8\u7f72\u5230arm\u673a\u5668\u4e0a\u8fdb\u884c\u6a21\u578b\u8c03\u7528\u63a8\u7406</p>"},{"location":"ai/llm/MLLM_Survey/","title":"MLLM (Multi-modal Large Language Model)","text":""},{"location":"ai/llm/MLLM_Survey/#1-llava","title":"1. LLAVA:","text":"<p>GitHub repo link:</p> <p>https://github.com/haotian-liu/LLaVA</p> <p>Project Page:</p> <p>https://llava-vl.github.io/</p> <p>Demo:</p> <p>https://llava.hliu.cc/</p>"},{"location":"ai/llm/MLLM_Survey/#11-about","title":"1.1 About:","text":"<p>[NeurIPS 2023 Oral] Visual Instruction Tuning: LLaVA (Large Language-and-Vision Assistant) built towards GPT-4V level capabilities.</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#12-model-zoo","title":"1.2 Model Zoo:","text":"<p>LLaVA-v1.5\u6a21\u578b\u6743\u91cd:</p> Version Size Schedule Checkpoint VQAv2 GQA VizWiz SQA T-VQA POPE MME MM-Bench MM-Bench-CN SEED LLaVA-Bench-Wild MM-Vet LLaVA-1.5 7B full_ft-1e liuhaotian/llava-v1.5-7b 78.5 62.0 50.0 66.8 58.2 85.9 1510.7 64.3 58.3 58.6 65.4 31.1 LLaVA-1.5 13B full_ft-1e liuhaotian/llava-v1.5-13b 80.0 63.3 53.6 71.6 61.3 85.9 1531.3 67.7 63.6 61.6 72.5 36.1 LLaVA-1.5 7B lora-1e liuhaotian/llava-v1.5-7b-lora 79.1 63.0 47.8 68.4 58.2 86.4 1476.9 66.1 58.9 60.1 67.9 30.2 LLaVA-1.5 13B lora-1e liuhaotian/llava-v1.5-13b-lora 80.0 63.3 58.9 71.2 60.2 86.7 1541.7 68.5 61.5 61.3 69.5 38.3 <p></p>"},{"location":"ai/llm/MLLM_Survey/#13-demo","title":"1.3 Demo\u6f14\u793a\uff1a","text":"<p>\u4e0b\u56fe\u662fLLaVA\u5bf9\u6bd4GPT-V\uff0cBLIP2\u53caOpenFlamingo\u7b49\u591a\u6a21\u6001\u89c6\u89c9\u5927\u6a21\u578b\uff0c\u53ef\u4ee5\u770b\u51faLLaVA\u5bf9\u56fe\u50cf\u7684\u7406\u89e3\u6bd4\u8f83\u6df1\u5165</p> <p></p> <p>\u4e0b\u9762\u4f7f\u7528\u81ea\u5df1\u7684\u56fe\u7247\u6765\u6d4b\u8bd5\u6027\u80fd\uff1a\u5bf9\u56fe\u50cf\u6574\u4f53\u7684\u628a\u63a7\u662f\u6bd4\u8f83\u5230\u4f4d\u7684\uff0c\u53ef\u4ee5\u51c6\u786e\u4e14\u8be6\u7ec6\u5730\u63cf\u8ff0\u56fe\u50cf\u4e2d\u7684\u573a\u666f\uff0c\u4f46\u662f\u5f53\u6211\u805a\u7126\u5230\u8be2\u95ee\u8f66\u724c\u662f\u591a\u5c11\u65f6\uff0c\u56de\u7b54\u6709\u4e00\u4e9b\u7455\u75b5\u3002</p> <p></p> <p>PS: \u652f\u6301\u4e2d\u6587\u95ee\u7b54\u4f46\u662f\u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u4e2d\u6587\u8bed\u5883\u4e0b\u7684\u80fd\u529b\u8fdc\u8fdc\u5f31\u4e8e\u82f1\u6587\u8bed\u5883</p>"},{"location":"ai/llm/MLLM_Survey/#2-minigpt-v2","title":"2. MiniGPT-v2","text":"<p>GitHub repo link:</p> <p>https://github.com/Vision-CAIR/MiniGPT-4</p> <p>Project Page:</p> <p>https://minigpt-v2.github.io/</p> <p>Demo:</p> <p>https://minigpt-v2.github.io/#</p>"},{"location":"ai/llm/MLLM_Survey/#21-about","title":"2.1 About:","text":"<p>Open-sourced codes for MiniGPT-4 and MiniGPT-v2. MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#22-model-zoo","title":"2.2 Model Zoo:","text":"<p>The pretrained LLM weights</p> <p>MiniGPT-v2 is based on Llama2 Chat 7B. For MiniGPT-4, we have both Vicuna V0 and Llama 2 version. Download the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.</p> Llama 2 Chat 7B Vicuna V0 13B Vicuna V0 7B Download Downlad Download <p>Then, set the variable llama_model in the model config file to the LLM weight path.</p> <ul> <li>For MiniGPT-v2, set the LLM path here at Line 14.</li> <li>For MiniGPT-4 (Llama2), set the LLM path here at Line 15.</li> <li>For MiniGPT-4 (Vicuna), set the LLM path here at Line 18</li> </ul> <p>The pretrained model checkpoints</p> <p>Download the pretrained model checkpoints</p> MiniGPT-v2 (after stage-2) MiniGPT-v2 (after stage-3) MiniGPT-v2 (online developing demo) Download Download Download <p>For MiniGPT-v2, set the path to the pretrained checkpoint in the evaluation config file in eval_configs/minigptv2_eval.yaml at Line 8.</p> MiniGPT-4 (Vicuna 13B) MiniGPT-4 (Vicuna 7B) MiniGPT-4 (LLaMA-2 Chat 7B) Download Download Download <p>For MiniGPT-4, set the path to the pretrained checkpoint in the evaluation config file in eval_configs/minigpt4_eval.yaml at Line 8 for Vicuna version or eval_configs/minigpt4_llama2_eval.yaml for LLama2 version.</p>"},{"location":"ai/llm/MLLM_Survey/#23-demo","title":"2.3 Demo\u6f14\u793a\uff1a","text":"<p><code>MiniGPT-v2 Demo</code>\u4e00\u5171\u652f\u6301<code>6</code>\u4e2a<code>Task</code>\uff1a<code>No Tag, Grounding, Refer, Detection, Identify, VQA</code>.</p> <p>For Abilities Involving Visual Grounding:</p> <ul> <li> <p>Grounding: CLICK Send to generate a grounded image description.</p> </li> <li> <p>Refer: Input a referring object and CLICK Send.</p> </li> <li> <p>Detection: Write a caption or phrase, and CLICK Send.</p> </li> <li> <p>Identify: Draw the bounding box on the uploaded image window and CLICK Send to generate the bounding box. (CLICK \"clear\" button before re-drawing next time).</p> </li> <li> <p>VQA: Input a visual question and CLICK Send.</p> </li> <li> <p>No Tag: Input whatever you want and CLICK Send without any tagging</p> </li> </ul> <p>You can also simply chat in free form!</p> <p>Demo\u4e2d\u4e5f\u7ed9\u51fa\u4e86\u4e00\u4e9b\u5b50\u4efb\u52a1\u7684\u4f8b\u5b50\uff0c\u53ef\u4ee5\u76f4\u63a5\u70b9\u51fb\u6765\u8fdb\u884c\u6d4b\u8bd5\uff1a</p> <p></p> <p>Detection\u529f\u80fd\u6f14\u793a\uff1a</p> <p>\u4f7f\u7528Detection\u529f\u80fd\u6765\u5bf9\u8f66\u724c\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u53ef\u4ee5\u770b\u5230\u5f88\u597d\u5730\u5b8c\u6210\u4e86\u8fd9\u4e2a\u4efb\u52a1\u3002</p> <p></p> <p>Grounding\u529f\u80fd\u5c55\u793a\uff1a</p> <p>\u8be5\u529f\u80fd\u662f\u5bf9\u56fe\u50cf\u7684\u573a\u666f\u505a\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790</p> <p></p> <p>Identify\u529f\u80fd\u5c55\u793a\uff1a</p> <p>\u5728\u539f\u56fe\u4e0a\u624b\u52a8\u5708\u51fa\u4e00\u4e2a\u7269\u4f53\uff0c\u7136\u540e\u6267\u884cIdentify\u529f\u80fd\uff0c\u53ef\u4ee5\u8bc6\u522b\u51fa\u5708\u51fa\u7684\u7269\u4f53\u662fcar</p> <p></p> <p>VQA\u529f\u80fd\u5c55\u793a\uff1a</p> <p>\u5f88\u660e\u663e\uff0cVQA\u7684\u529f\u80fd\u4e0d\u5982LLaVA\uff0c\u4f46\u662f\u6bd4LLaVA\u591a\u4e86\u76ee\u6807\u68c0\u6d4b\u7b49\u529f\u80fd</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#3-fuyu-8b-transformer","title":"3. fuyu-8b -- Transformer\u4e00\u4f5c","text":"<p>Huggingface repo link:</p> <p>https://huggingface.co/adept/fuyu-8b</p>"},{"location":"ai/llm/MLLM_Survey/#31-about","title":"3.1 About\uff1a","text":"<p>\u4ece\u5b98\u65b9\u9875\u9762\u7684\u4ecb\u7ecd\u6765\u770b\uff0c\u8be5\u6a21\u578b\u5e76\u4e0d\u5177\u5907\u7cbe\u7ec6\u5316\u89c6\u89c9\u80fd\u529b\uff0c\u5b83\u7684\u5207\u5165\u70b9\u5728\u4e8e\u901f\u5ea6\u5feb\uff0c\u6613\u4e8e\u5728\u6d88\u8d39\u7ea7\u4ea7\u54c1\u4e0a\u4f7f\u7528\uff0c\u4e2a\u4eba\u89c9\u5f97\u5e94\u8be5\u4e0d\u7b26\u5408\u6211\u4eec\u9879\u76ee\u7684\u9700\u6c42</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#32-benchmarks","title":"3.2 Benchmarks:","text":"<p>\u6839\u636e\u7ed9\u51fa\u7684\u5728benchmark\u4e0a\u7684\u6570\u636e\uff0cFuyu-8B\u5728AI2D\u4e0a\u8fbe\u5230\u4e86SOTA\uff0c\u4f46\u662f\u6211\u4eec\u9700\u8981\u7684\u662fVQA\u80fd\u529b</p> val Task Fuyu-8B Fuyu-Medium LLaVA 1.5 (13.5B) QWEN-VL (10B) PALI-X (55B) PALM-e-12B PALM-e-562B VQAv2 74.2 77.4 80 79.5 86.1 76.2 80.0 OKVQA 60.6 63.1 n/a 58.6 66.1 55.5 66.1 COCO Captions 141 138 n/a n/a 149 135 138 AI2D 64.5 73.7 n/a 62.3 81.2 n/a n/a"},{"location":"ai/llm/MLLM_Survey/#33-how-to-use","title":"3.3 How to use:","text":"<p>load the model and perform inference as follows:</p> <pre><code>from transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\n\n# load model and processor\nmodel_id = \"adept/fuyu-8b\"\nprocessor = FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map=\"cuda:0\")\n\n# prepare inputs for the model\ntext_prompt = \"Generate a coco-style caption.\\n\"\nimage_path = \"bus.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png\nimage = Image.open(image_path)\n\ninputs = processor(text=text_prompt, images=image, return_tensors=\"pt\")\nfor k, v in inputs.items():\n    inputs[k] = v.to(\"cuda:0\")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs, max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\nassert generation_text == ['A bus parked on the side of a road.']\n</code></pre> <p>Fuyu can also perform some question answering on natural images and charts/diagrams (thought fine-tuning may be required for good performance):</p> <pre><code>text_prompt = \"What color is the bus?\\n\"\nimage_path = \"bus.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png\nimage_pil = Image.open(image_path)\n\nmodel_inputs = processor(text=text_prompt, images=[image_pil], device=\"cuda:0\")\nfor k, v in model_inputs.items():\n    model_inputs[k] = v.to(\"cuda:0\")\n\ngeneration_output = model.generate(**model_inputs, max_new_tokens=6)\ngeneration_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)\nassert generation_text == [\"The bus is blue.\\n\"]\n\n\ntext_prompt = \"What is the highest life expectancy at birth of male?\\n\"\nimage_path = \"chart.png\"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png\nimage_pil = Image.open(image_path)\n\nmodel_inputs = processor(text=text_prompt, images=[image_pil], device=\"cuda:0\")\nfor k, v in model_inputs.items():\n    model_inputs[k] = v.to(\"cuda:0\")\n\ngeneration_output = model.generate(**model_inputs, max_new_tokens=16)\ngeneration_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)\nassert generation_text == [\"The life expectancy at birth of males in 2018 is 80.7.\\n\"]\n</code></pre>"},{"location":"ai/llm/MLLM_Survey/#4-pali-3","title":"4. PALI-3","text":"<p>\u8c37\u6b4c\u53d1\u5e03\u76845B\u53c2\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578bPaLI-3\uff0c1/10\u4f53\u91cf\u5c31\u8fbe\u5230SOTA\uff0c\u66f4\u5c0f\u66f4\u5feb\u4e14\u66f4\u5f3a\uff0c\u4f46\u662f\u4e0d\u5f00\u6e90</p> <p></p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#5-cogvlm","title":"5. \u6e05\u534e\u667a\u8c31CogVLM","text":"<p>GitHub repo link:</p> <p>https://github.com/THUDM/CogVLM</p> <p>Project Page:</p> <p>https://chatglm.cn/</p> <p>Demo:</p> <p>http://36.103.203.44:7861/</p>"},{"location":"ai/llm/MLLM_Survey/#51-about","title":"5.1 About:","text":"<ul> <li>CogVLM \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u3002CogVLM-17B \u62e5\u6709 100 \u4ebf\u89c6\u89c9\u53c2\u6570\u548c 70 \u4ebf\u8bed\u8a00\u53c2\u6570\u3002</li> <li>CogVLM-17B \u5728 10 \u4e2a\u7ecf\u5178\u8de8\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86 SOTA \u6027\u80fd\uff0c\u5305\u62ec NoCaps\u3001Flicker30k captioning\u3001RefCOCO\u3001RefCOCO+\u3001RefCOCOg\u3001Visual7W\u3001GQA\u3001ScienceQA\u3001VizWiz VQA \u548c TDIUC\uff0c\u800c\u5728 VQAv2\u3001OKVQA\u3001TextVQA\u3001COCO captioning \u7b49\u65b9\u9762\u5219\u6392\u540d\u7b2c\u4e8c\uff0c\u8d85\u8d8a\u6216\u4e0e PaLI-X 55B \u6301\u5e73\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u7ebf\u4e0a demo \u4f53\u9a8c CogVLM \u591a\u6a21\u6001\u5bf9\u8bdd\u3002</li> </ul>"},{"location":"ai/llm/MLLM_Survey/#52","title":"5.2 \u5bf9\u6bd4\uff1a","text":"<ul> <li>CogVLM \u80fd\u591f\u51c6\u786e\u5730\u63cf\u8ff0\u56fe\u50cf\uff0c\u51e0\u4e4e\u4e0d\u4f1a\u51fa\u73b0\u5e7b\u89c9\u3002</li> </ul> <p>\u4e0eLLAVA-1.5 \u548c MiniGPT-4 \u7684\u6bd4\u8f83:</p> <p>https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/llava-comparison-min.png</p> <ul> <li>CogVLM \u80fd\u7406\u89e3\u548c\u56de\u7b54\u5404\u79cd\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u5e76\u6709\u4e00\u4e2a\u89c6\u89c9\u5b9a\u4f4d\u7248\u672c\u3002</li> </ul> <p></p> <ul> <li>CogVLM \u6709\u65f6\u6bd4 GPT-4V(ision) \u63d0\u53d6\u5230\u66f4\u591a\u7684\u7ec6\u8282\u4fe1\u606f\u3002</li> </ul> <p></p>"},{"location":"ai/llm/MLLM_Survey/#53-demo","title":"5.3 Demo\u6f14\u793a\uff1a","text":"<p>\u8fd9\u662f\u6700\u63a5\u8fd1\u771f\u5b9e\u8f66\u724c\u7684\u56de\u7b54\uff0c\u53ea\u9519\u4e86\u4e00\u4e2a\u5b57\u6bcd</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#6-gpt4-vision","title":"6. GPT4-Vision","text":"<p>\u5f88\u5f3a\uff0c\u4f46CloseAI</p>"},{"location":"ai/llm/MLLM_Survey/#7-palm-e","title":"7. PALM-E","text":"<p>\u5f88\u5f3a\uff0c\u4f46\u4e0d\u5f00\u6e90</p>"},{"location":"ai/llm/MLLM_Survey/#8-som-gpt4v","title":"8. SoM-GPT4V","text":"<p>GitHub repo link:</p> <p>https://github.com/microsoft/SoM</p> <p>Project Page:</p> <p>https://som-gpt4v.github.io/</p>"},{"location":"ai/llm/MLLM_Survey/#81-about","title":"8.1 About:","text":"<p>Set-of-Mark Prompting for LMMs. Set-of-Mark Prompting or GPT-4V - Visual Prompting for Vision!</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#82-quick-start","title":"8.2 Quick Start:","text":""},{"location":"ai/llm/MLLM_Survey/#quick-start","title":"\ud83d\ude80 Quick Start","text":"<ul> <li>Install segmentation packages</li> </ul> <pre><code># install SEEM\npip install git+https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git@package\n# install SAM\npip install git+https://github.com/facebookresearch/segment-anything.git\n# install Semantic-SAM\npip install git+https://github.com/UX-Decoder/Semantic-SAM.git@package\n</code></pre> <ul> <li>Download the pretrained models</li> </ul> <pre><code>sh download_ckpt.sh\n</code></pre> <ul> <li>Run the demo</li> </ul> <pre><code>python demo_som.py\n</code></pre> <p>And you will see this interface:</p> <p></p>"},{"location":"ai/llm/MLLM_Survey/#82-demo","title":"8.2 Demo\u6f14\u793a\uff1a","text":"<p>\u8be5\u9879\u76ee\u7684\u4e3b\u8981\u529f\u80fd\u4e0d\u662f\u4ece\u6a21\u578b\u63a8\u7406\u7aef\u505a\u89c6\u89c9\u7b97\u6cd5\u7684\u6539\u8fdb\uff0c\u800c\u662f\u4ece\u6570\u636e\u7aef\u505a\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\uff0c\u6bd4\u5982\u7ed9\u51fa\u4e00\u5f20\u56fe\uff0cSoM\u5c31\u53ef\u4ee5\u8f93\u51fa\u4e00\u4e2a\u5e26\u6709\u5f88\u591a\u7279\u5f81\u6807\u8bb0\u7684\u7279\u5f81\u56fe\uff0c\u800c\u8fd9\u79cd\u56fe\u5bf9\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u8fdb\u884cVQA\u548cImage Caption\u662f\u975e\u5e38\u6709\u5e2e\u52a9\u7684\uff0c\u53ef\u4ee5\u5e2e\u52a9\u89c6\u89c9\u6a21\u578b\u5145\u5206\u91ca\u653e\u5b83\u7684\u80fd\u529b\uff1a</p> <p>Example 1\uff1a</p> <p></p> <p>Example 2\uff1a</p> <p></p> <p>\u81ea\u5df1\u4e0a\u4f20\u56fe\u50cf\u8fdb\u884c\u56fe\u50cf\u589e\u5f3a\u6807\u8bb0\uff1a</p> <p>\u6548\u679c\u770b\u8d77\u6765\u786e\u5b9e\u5f88\u597d\uff0c\u4e4b\u540e\u6211\u4eec\u8fdb\u884c\u6f14\u793a\u7684\u56fe\u50cf\u53ef\u4ee5\u5148\u4f7f\u7528SoM\u8fdb\u884c\u56fe\u50cf\u4fe1\u606f\u6807\u8bb0\u589e\u5f3a\uff0c\u4e4b\u540e\u518d\u8f93\u5165\u5230\u89c6\u89c9\u6a21\u578b\u4e2d\uff0c\u8fd9\u6837\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u6a21\u578b\u6548\u679c\u3002</p> <p></p>"},{"location":"ai/llm/rag_vs_finetune/","title":"RAG vs Fine-tuning","text":""},{"location":"ai/llm/rag_vs_finetune/#_1","title":"\u77e5\u8bc6\u66f4\u65b0","text":"<ul> <li>RAG\uff1a\u76f4\u63a5\u66f4\u65b0\u68c0\u7d22\u77e5\u8bc6\u5e93\uff0c\u4fe1\u606f\u5b9e\u65f6\u66f4\u65b0\uff0c\u65e0\u9700\u9891\u7e41\u518d\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u53d8\u66f4\u6570\u636e\u73af\u5883\u3002</li> <li>Fine-Tuning\uff1a\u5b58\u50a8\u9759\u6001\u6570\u636e\uff0c\u9700\u91cd\u65b0\u8bad\u7ec3\u624d\u80fd\u66f4\u65b0\u77e5\u8bc6\u548c\u6570\u636e\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_2","title":"\u5916\u90e8\u77e5\u8bc6","text":"<ul> <li>RAG\uff1a\u64c5\u957f\u5229\u7528\u5916\u90e8\u8d44\u6e90\uff0c\u9002\u5408\u8bbf\u95ee\u6587\u6863\u6216\u5176\u4ed6\u7ed3\u6784\u5316/\u975e\u7ed3\u6784\u5316\u6570\u636e\u5e93\u3002</li> <li>Fine-Tuning\uff1a\u53ef\u4ee5\u5c06\u4ece\u9884\u8bad\u7ec3\u4e2d\u83b7\u5f97\u7684\u77e5\u8bc6\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u4f46\u5bf9\u9891\u7e41\u53d8\u5316\u7684\u6570\u636e\u6e90\u53ef\u80fd\u4e0d\u592a\u5b9e\u7528\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_3","title":"\u6570\u636e\u5904\u7406","text":"<ul> <li>RAG\uff1a\u6d89\u53ca\u6700\u5c11\u7684\u6570\u636e\u5904\u7406\u548c\u5904\u7406\u3002</li> <li>Fine-Tuning\uff1a\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u521b\u5efa\uff0c\u6570\u636e\u96c6\u6709\u9650\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u63d0\u5347\u6709\u9650\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_4","title":"\u6a21\u578b\u5b9a\u5236","text":"<ul> <li>RAG\uff1a\u4e13\u6ce8\u4e8e\u4fe1\u606f\u68c0\u7d22\u548c\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u53ef\u80fd\u65e0\u6cd5\u5b8c\u5168\u5b9a\u5236\u6a21\u578b\u884c\u4e3a\u6216\u5199\u4f5c\u98ce\u683c\u3002</li> <li>Fine-Tuning\uff1a\u5141\u8bb8\u8c03\u6574LLM\u884c\u4e3a\u3001\u5199\u4f5c\u98ce\u683c\u6216\u57fa\u4e8e\u7279\u5b9a\u672f\u8bed\u7684\u9886\u57df\u77e5\u8bc6\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_5","title":"\u53ef\u89e3\u91ca\u6027","text":"<ul> <li>RAG\uff1a\u54cd\u5e94\u53ef\u4ee5\u8ffd\u6eaf\u5230\u5177\u4f53\u6570\u636e\u6e90\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002</li> <li>Fine-Tuning\uff1a\u7c7b\u4f3c\u4e8e\u9ed1\u7bb1\uff0c\u6a21\u578b\u4e3a\u4f55\u4ee5\u67d0\u79cd\u65b9\u5f0f\u53cd\u5e94\u4e0d\u603b\u662f\u6e05\u695a\uff0c\u5bfc\u81f4\u76f8\u5bf9\u8f83\u4f4e\u7684\u53ef\u89e3\u91ca\u6027\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_6","title":"\u8ba1\u7b97\u8d44\u6e90","text":"<ul> <li>RAG\uff1a\u4f9d\u8d56\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6765\u652f\u6301\u4e0e\u6570\u636e\u5e93\u76f8\u5173\u7684\u68c0\u7d22\u7b56\u7565\u548c\u6280\u672f\uff0c\u5916\u90e8\u6570\u636e\u6e90\u96c6\u6210\u548c\u66f4\u65b0\u9700\u8981\u7ef4\u62a4\u3002</li> <li>Fine-Tuning\uff1a\u9700\u8981\u51c6\u5907\u548c\u7ba1\u7406\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u96c6\u3001\u5b9a\u4e49\u5fae\u8c03\u76ee\u6807\uff0c\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u8ba1\u7b97\u8d44\u6e90\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_7","title":"\u5ef6\u8fdf\u8981\u6c42","text":"<ul> <li>RAG\uff1a\u6d89\u53ca\u6570\u636e\u68c0\u7d22\uff0c\u53ef\u80fd\u5bfc\u81f4\u66f4\u9ad8\u7684\u5ef6\u8fdf\u3002</li> <li>Fine-Tuning\uff1a\u7ecf\u8fc7\u5fae\u8c03\u7684LLM\u65e0\u9700\u68c0\u7d22\uff0c\u53ef\u5728\u8f83\u4f4e\u5ef6\u8fdf\u4e0b\u54cd\u5e94\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_8","title":"\u51cf\u5c11\u5e7b\u89c9","text":"<ul> <li>RAG\uff1a\u6bcf\u4e2a\u7b54\u6848\u57fa\u4e8e\u68c0\u7d22\u5230\u7684\u8bc1\u636e\uff0c\u5e7b\u89c9\u6982\u7387\u8f83\u4f4e\u3002</li> <li>Fine-Tuning\uff1a\u901a\u8fc7\u57fa\u4e8e\u7279\u5b9a\u9886\u57df\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u51cf\u5c11\u5e7b\u89c9\uff0c\u4f46\u5728\u9762\u5bf9\u4e0d\u719f\u6089\u7684\u8f93\u5165\u65f6\u4ecd\u53ef\u80fd\u51fa\u73b0\u5e7b\u89c9\u3002</li> </ul>"},{"location":"ai/llm/rag_vs_finetune/#_9","title":"\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898","text":"<ul> <li>RAG\uff1a\u4ece\u5916\u90e8\u6570\u636e\u5e93\u68c0\u7d22\u6587\u672c\u5f15\u53d1\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\u3002</li> <li>Fine-Tuning\uff1a\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u654f\u611f\u5185\u5bb9\uff0c\u53ef\u80fd\u51fa\u73b0\u4f26\u7406\u548c\u9690\u79c1\u95ee\u9898\u3002</li> </ul>"}]}