# MLLM (Multi-modal Large Language Model)

## 1. LLAVA:

**GitHub repo link:**

https://github.com/haotian-liu/LLaVA

**Project Page:**

https://llava-vl.github.io/

**Demo:**

https://llava.hliu.cc/

### 1.1 About:

**[NeurIPS 2023 Oral]** Visual Instruction Tuning: LLaVA (Large Language-and-Vision Assistant) built towards GPT-4V level capabilities.

![image-20231027174819999](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027174819999.png)

### 1.2 Model Zoo:

**LLaVA-v1.5æ¨¡å‹æƒé‡:**

| Version   | Size | Schedule   | Checkpoint                                                   | VQAv2 | GQA  | VizWiz | SQA  | T-VQA | POPE | MME    | MM-Bench | MM-Bench-CN | SEED | LLaVA-Bench-Wild | MM-Vet |
| --------- | ---- | ---------- | ------------------------------------------------------------ | ----- | ---- | ------ | ---- | ----- | ---- | ------ | -------- | ----------- | ---- | ---------------- | ------ |
| LLaVA-1.5 | 7B   | full_ft-1e | [liuhaotian/llava-v1.5-7b](https://huggingface.co/liuhaotian/llava-v1.5-7b) | 78.5  | 62.0 | 50.0   | 66.8 | 58.2  | 85.9 | 1510.7 | 64.3     | 58.3        | 58.6 | 65.4             | 31.1   |
| LLaVA-1.5 | 13B  | full_ft-1e | [liuhaotian/llava-v1.5-13b](https://huggingface.co/liuhaotian/llava-v1.5-13b) | 80.0  | 63.3 | 53.6   | 71.6 | 61.3  | 85.9 | 1531.3 | 67.7     | 63.6        | 61.6 | 72.5             | 36.1   |
| LLaVA-1.5 | 7B   | lora-1e    | [liuhaotian/llava-v1.5-7b-lora](https://huggingface.co/liuhaotian/llava-v1.5-7b-lora) | 79.1  | 63.0 | 47.8   | 68.4 | 58.2  | 86.4 | 1476.9 | 66.1     | 58.9        | 60.1 | 67.9             | 30.2   |
| LLaVA-1.5 | 13B  | lora-1e    | [liuhaotian/llava-v1.5-13b-lora](https://huggingface.co/liuhaotian/llava-v1.5-13b-lora) | 80.0  | 63.3 | 58.9   | 71.2 | 60.2  | 86.7 | 1541.7 | 68.5     | 61.5        | 61.3 | 69.5             | 38.3   |

![image-20231027175117102](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027175117102.png)

### 1.3 Demoæ¼”ç¤ºï¼š

ä¸‹å›¾æ˜¯LLaVAå¯¹æ¯”GPT-Vï¼ŒBLIP2åŠOpenFlamingoç­‰å¤šæ¨¡æ€è§†è§‰å¤§æ¨¡å‹ï¼Œå¯ä»¥çœ‹å‡ºLLaVAå¯¹å›¾åƒçš„ç†è§£æ¯”è¾ƒæ·±å…¥

![img](https://llava-vl.github.io/images/cmp_ironing.png)

ä¸‹é¢ä½¿ç”¨è‡ªå·±çš„å›¾ç‰‡æ¥æµ‹è¯•æ€§èƒ½ï¼šå¯¹å›¾åƒæ•´ä½“çš„æŠŠæ§æ˜¯æ¯”è¾ƒåˆ°ä½çš„ï¼Œå¯ä»¥å‡†ç¡®ä¸”è¯¦ç»†åœ°æè¿°å›¾åƒä¸­çš„åœºæ™¯ï¼Œä½†æ˜¯å½“æˆ‘èšç„¦åˆ°è¯¢é—®è½¦ç‰Œæ˜¯å¤šå°‘æ—¶ï¼Œå›ç­”æœ‰ä¸€äº›ç‘•ç–µã€‚

![image-20231027182040727](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027182040727.png)

**PS: æ”¯æŒä¸­æ–‡é—®ç­”ä½†æ˜¯ç»è¿‡æµ‹è¯•ï¼Œä¸­æ–‡è¯­å¢ƒä¸‹çš„èƒ½åŠ›è¿œè¿œå¼±äºè‹±æ–‡è¯­å¢ƒ**

## 2. MiniGPT-v2

**GitHub repo link:**

https://github.com/Vision-CAIR/MiniGPT-4

**Project Page:**

https://minigpt-v2.github.io/

**Demo:**

https://minigpt-v2.github.io/#

### 2.1 About:

Open-sourced codes for MiniGPT-4 and MiniGPT-v2. **MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning**

![image-20231027182555791](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027182555791.png)

### 2.2 Model Zoo:

**The pretrained LLM weights**

**MiniGPT-v2** is based on Llama2 Chat 7B. For **MiniGPT-4**, we have both Vicuna V0 and Llama 2 version. Download the corresponding LLM weights from the following huggingface space via clone the repository using git-lfs.

| Llama 2 Chat 7B                                              | Vicuna V0 13B                                                | Vicuna V0 7B                                                 |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Download](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main) | [Downlad](https://huggingface.co/Vision-CAIR/vicuna/tree/main) | [Download](https://huggingface.co/Vision-CAIR/vicuna-7b/tree/main) |

Then, set the variable *llama_model* in the model config file to the LLM weight path.

- For MiniGPT-v2, set the LLM path [here](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/minigpt4/configs/models/minigpt_v2.yaml#L15) at Line 14.
- For MiniGPT-4 (Llama2), set the LLM path [here](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/minigpt4/configs/models/minigpt4_llama2.yaml#L15) at Line 15.
- For MiniGPT-4 (Vicuna), set the LLM path [here](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/minigpt4/configs/models/minigpt4_vicuna0.yaml#L18) at Line 18

**The pretrained model checkpoints**

Download the pretrained model checkpoints

| MiniGPT-v2 (after stage-2)                                   | MiniGPT-v2 (after stage-3)                                   | MiniGPT-v2 (online developing demo)                          |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Download](https://drive.google.com/file/d/1Vi_E7ZtZXRAQcyz4f8E6LtLh2UXABCmu/view?usp=sharing) | [Download](https://drive.google.com/file/d/1jAbxUiyl04SFJMN4sF1vvUU69Etuz4qa/view?usp=sharing) | [Download](https://drive.google.com/file/d/1aVbfW7nkCSYx99_vCRyP1sOlQiWVSnAl/view?usp=sharing) |

For **MiniGPT-v2**, set the path to the pretrained checkpoint in the evaluation config file in [eval_configs/minigptv2_eval.yaml](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/eval_configs/minigptv2_eval.yaml#L10) at Line 8.

| MiniGPT-4 (Vicuna 13B)                                       | MiniGPT-4 (Vicuna 7B)                                        | MiniGPT-4 (LLaMA-2 Chat 7B)                                  |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [Download](https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view?usp=share_link) | [Download](https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing) | [Download](https://drive.google.com/file/d/11nAPjEok8eAGGEG1N2vXo3kBLCg0WgUk/view?usp=sharing) |

For **MiniGPT-4**, set the path to the pretrained checkpoint in the evaluation config file in [eval_configs/minigpt4_eval.yaml](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/eval_configs/minigpt4_eval.yaml#L10) at Line 8 for Vicuna version or [eval_configs/minigpt4_llama2_eval.yaml](https://github.com/Vision-CAIR/MiniGPT-4/blob/main/eval_configs/minigpt4_llama2_eval.yaml#L10) for LLama2 version.

### 2.3 Demoæ¼”ç¤ºï¼š

`MiniGPT-v2 Demo`ä¸€å…±æ”¯æŒ`6`ä¸ª`Task`ï¼š`No Tag, Grounding, Refer, Detection, Identify, VQA`.

For Abilities Involving Visual Grounding:

- Grounding: CLICK **Send** to generate a grounded image description.

- Refer: Input a referring object and CLICK **Send**.

- Detection: Write a caption or phrase, and CLICK **Send**.

- Identify: Draw the bounding box on the uploaded image window and CLICK **Send** to generate the bounding box. (CLICK "clear" button before re-drawing next time).

- VQA: Input a visual question and CLICK **Send**.

- No Tag: Input whatever you want and CLICK **Send** without any tagging

You can also simply chat in free form!



Demoä¸­ä¹Ÿç»™å‡ºäº†ä¸€äº›å­ä»»åŠ¡çš„ä¾‹å­ï¼Œå¯ä»¥ç›´æ¥ç‚¹å‡»æ¥è¿›è¡Œæµ‹è¯•ï¼š

![image-20231027183207289](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027183207289.png)

**DetectionåŠŸèƒ½æ¼”ç¤ºï¼š**

ä½¿ç”¨DetectionåŠŸèƒ½æ¥å¯¹è½¦ç‰Œè¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¯ä»¥çœ‹åˆ°å¾ˆå¥½åœ°å®Œæˆäº†è¿™ä¸ªä»»åŠ¡ã€‚

![image-20231027183438316](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027183438316.png)

**GroundingåŠŸèƒ½å±•ç¤ºï¼š**

è¯¥åŠŸèƒ½æ˜¯å¯¹å›¾åƒçš„åœºæ™¯åšä¸€ä¸ªå…¨é¢çš„åˆ†æ

![image-20231027184204112](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027184204112.png)

**IdentifyåŠŸèƒ½å±•ç¤ºï¼š**

åœ¨åŸå›¾ä¸Šæ‰‹åŠ¨åœˆå‡ºä¸€ä¸ªç‰©ä½“ï¼Œç„¶åæ‰§è¡ŒIdentifyåŠŸèƒ½ï¼Œå¯ä»¥è¯†åˆ«å‡ºåœˆå‡ºçš„ç‰©ä½“æ˜¯car

![image-20231027184634515](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027184634515.png)

**VQAåŠŸèƒ½å±•ç¤ºï¼š**

å¾ˆæ˜æ˜¾ï¼ŒVQAçš„åŠŸèƒ½ä¸å¦‚LLaVAï¼Œä½†æ˜¯æ¯”LLaVAå¤šäº†ç›®æ ‡æ£€æµ‹ç­‰åŠŸèƒ½

![image-20231027185059064](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027185059064.png)

## 3. fuyu-8b -- Transformerä¸€ä½œ

**Huggingface repo link:**

https://huggingface.co/adept/fuyu-8b

### 3.1 Aboutï¼š

ä»å®˜æ–¹é¡µé¢çš„ä»‹ç»æ¥çœ‹ï¼Œè¯¥æ¨¡å‹å¹¶ä¸å…·å¤‡ç²¾ç»†åŒ–è§†è§‰èƒ½åŠ›ï¼Œå®ƒçš„åˆ‡å…¥ç‚¹åœ¨äºé€Ÿåº¦å¿«ï¼Œæ˜“äºåœ¨æ¶ˆè´¹çº§äº§å“ä¸Šä½¿ç”¨ï¼Œä¸ªäººè§‰å¾—åº”è¯¥ä¸ç¬¦åˆæˆ‘ä»¬é¡¹ç›®çš„éœ€æ±‚

![image-20231027185559604](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027185559604.png)

### 3.2 Benchmarks:

æ ¹æ®ç»™å‡ºçš„åœ¨benchmarkä¸Šçš„æ•°æ®ï¼ŒFuyu-8Båœ¨AI2Dä¸Šè¾¾åˆ°äº†SOTAï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦çš„æ˜¯VQAèƒ½åŠ›

| val Task      | Fuyu-8B | Fuyu-Medium | LLaVA 1.5 (13.5B) | QWEN-VL (10B) | PALI-X (55B) | PALM-e-12B | PALM-e-562B |
| ------------- | ------- | ----------- | ----------------- | ------------- | ------------ | ---------- | ----------- |
| VQAv2         | 74.2    | 77.4        | 80                | 79.5          | 86.1         | 76.2       | 80.0        |
| OKVQA         | 60.6    | 63.1        | n/a               | 58.6          | 66.1         | 55.5       | 66.1        |
| COCO Captions | 141     | 138         | n/a               | n/a           | 149          | 135        | 138         |
| AI2D          | 64.5    | 73.7        | n/a               | 62.3          | 81.2         | n/a        | n/a         |

### 3.3 How to use:

**load the model and perform inference as follows:**

```python
from transformers import FuyuProcessor, FuyuForCausalLM
from PIL import Image

# load model and processor
model_id = "adept/fuyu-8b"
processor = FuyuProcessor.from_pretrained(model_id)
model = FuyuForCausalLM.from_pretrained(model_id, device_map="cuda:0")

# prepare inputs for the model
text_prompt = "Generate a coco-style caption.\n"
image_path = "bus.png"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png
image = Image.open(image_path)

inputs = processor(text=text_prompt, images=image, return_tensors="pt")
for k, v in inputs.items():
    inputs[k] = v.to("cuda:0")

# autoregressively generate text
generation_output = model.generate(**inputs, max_new_tokens=7)
generation_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)
assert generation_text == ['A bus parked on the side of a road.']
```

**Fuyu can also perform some question answering on natural images and charts/diagrams (thought fine-tuning may be required for good performance):**

```python
text_prompt = "What color is the bus?\n"
image_path = "bus.png"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/bus.png
image_pil = Image.open(image_path)

model_inputs = processor(text=text_prompt, images=[image_pil], device="cuda:0")
for k, v in model_inputs.items():
    model_inputs[k] = v.to("cuda:0")

generation_output = model.generate(**model_inputs, max_new_tokens=6)
generation_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)
assert generation_text == ["The bus is blue.\n"]


text_prompt = "What is the highest life expectancy at birth of male?\n"
image_path = "chart.png"  # https://huggingface.co/adept-hf-collab/fuyu-8b/blob/main/chart.png
image_pil = Image.open(image_path)

model_inputs = processor(text=text_prompt, images=[image_pil], device="cuda:0")
for k, v in model_inputs.items():
    model_inputs[k] = v.to("cuda:0")

generation_output = model.generate(**model_inputs, max_new_tokens=16)
generation_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)
assert generation_text == ["The life expectancy at birth of males in 2018 is 80.7.\n"]
```



## 4. PALI-3

è°·æ­Œå‘å¸ƒçš„5Bå‚æ•°è§†è§‰è¯­è¨€æ¨¡å‹PaLI-3ï¼Œ1/10ä½“é‡å°±è¾¾åˆ°SOTAï¼Œæ›´å°æ›´å¿«ä¸”æ›´å¼ºï¼Œä½†æ˜¯**ä¸å¼€æº**

![image-20231027190859678](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027190859678.png)

![image-20231027190923956](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027190923956.png)



## 5. æ¸…åæ™ºè°±CogVLM

**GitHub repo link:**

https://github.com/THUDM/CogVLM

**Project Page:**

https://chatglm.cn/

**Demo:**

http://36.103.203.44:7861/

### 5.1 About:

- CogVLM æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚CogVLM-17B æ‹¥æœ‰ 100 äº¿è§†è§‰å‚æ•°å’Œ 70 äº¿è¯­è¨€å‚æ•°ã€‚
- CogVLM-17B åœ¨ 10 ä¸ªç»å…¸è·¨æ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šå–å¾—äº† SOTA æ€§èƒ½ï¼ŒåŒ…æ‹¬ NoCapsã€Flicker30k captioningã€RefCOCOã€RefCOCO+ã€RefCOCOgã€Visual7Wã€GQAã€ScienceQAã€VizWiz VQA å’Œ TDIUCï¼Œè€Œåœ¨ VQAv2ã€OKVQAã€TextVQAã€COCO captioning ç­‰æ–¹é¢åˆ™æ’åç¬¬äºŒï¼Œè¶…è¶Šæˆ–ä¸ PaLI-X 55B æŒå¹³ã€‚æ‚¨å¯ä»¥é€šè¿‡çº¿ä¸Š [demo](http://36.103.203.44:7861/) ä½“éªŒ CogVLM å¤šæ¨¡æ€å¯¹è¯ã€‚

![img](https://github.com/THUDM/CogVLM/raw/main/assets/metrics-min.png)

### 5.2 å¯¹æ¯”ï¼š

- CogVLM èƒ½å¤Ÿå‡†ç¡®åœ°æè¿°å›¾åƒï¼Œ**å‡ ä¹ä¸ä¼šå‡ºç°å¹»è§‰**ã€‚

 **ä¸LLAVA-1.5 å’Œ MiniGPT-4 çš„æ¯”è¾ƒ:**

https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/llava-comparison-min.png

- CogVLM èƒ½ç†è§£å’Œå›ç­”å„ç§ç±»å‹çš„é—®é¢˜ï¼Œå¹¶æœ‰ä¸€ä¸ª**è§†è§‰å®šä½**ç‰ˆæœ¬ã€‚

![img](https://github.com/THUDM/CogVLM/raw/main/assets/pear_grounding.png)

- CogVLM æœ‰æ—¶æ¯” GPT-4V(ision) æå–åˆ°æ›´å¤šçš„ç»†èŠ‚ä¿¡æ¯ã€‚

![img](https://github.com/THUDM/CogVLM/raw/main/assets/compare-min.png)

### 5.3 Demoæ¼”ç¤ºï¼š

è¿™æ˜¯æœ€æ¥è¿‘çœŸå®è½¦ç‰Œçš„å›ç­”ï¼Œåªé”™äº†ä¸€ä¸ªå­—æ¯

![image-20231027192713229](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027192713229.png)

## 6. GPT4-Vision

**å¾ˆå¼ºï¼Œä½†CloseAI**

## 7. PALM-E

**å¾ˆå¼ºï¼Œä½†ä¸å¼€æº**

## 8. SoM-GPT4V

**GitHub repo link:**

https://github.com/microsoft/SoM

**Project Page:**

https://som-gpt4v.github.io/

### 8.1 About:

Set-of-Mark Prompting for LMMs. [Set-of-Mark Prompting or GPT-4V - Visual Prompting for Vision!](https://github.com/microsoft/SoM#set-of-mark-prompting-or-gpt-4v---visual-prompting-for-vision)

![image-20231027193206520](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027193206520.png)

### 8.2 Quick Start:

### [ğŸš€ Quick Start](https://github.com/microsoft/SoM#rocket-quick-start)

- Install segmentation packages

```bash
# install SEEM
pip install git+https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git@package
# install SAM
pip install git+https://github.com/facebookresearch/segment-anything.git
# install Semantic-SAM
pip install git+https://github.com/UX-Decoder/Semantic-SAM.git@package
```

- Download the pretrained models

```bash
sh download_ckpt.sh
```

- Run the demo

```python
python demo_som.py
```

And you will see this interface:

![image-20231027193257134](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027193257134.png)

### 8.2 Demoæ¼”ç¤ºï¼š

è¯¥é¡¹ç›®çš„ä¸»è¦åŠŸèƒ½ä¸æ˜¯ä»æ¨¡å‹æ¨ç†ç«¯åšè§†è§‰ç®—æ³•çš„æ”¹è¿›ï¼Œè€Œæ˜¯ä»æ•°æ®ç«¯åšæ•°æ®å¢å¼ºæ“ä½œï¼Œæ¯”å¦‚ç»™å‡ºä¸€å¼ å›¾ï¼ŒSoMå°±å¯ä»¥è¾“å‡ºä¸€ä¸ªå¸¦æœ‰å¾ˆå¤šç‰¹å¾æ ‡è®°çš„ç‰¹å¾å›¾ï¼Œè€Œè¿™ç§å›¾å¯¹äºè§†è§‰è¯­è¨€æ¨¡å‹æ¥è¿›è¡ŒVQAå’ŒImage Captionæ˜¯éå¸¸æœ‰å¸®åŠ©çš„ï¼Œå¯ä»¥å¸®åŠ©è§†è§‰æ¨¡å‹å……åˆ†é‡Šæ”¾å®ƒçš„èƒ½åŠ›ï¼š

**Example 1ï¼š**

![image-20231027193555709](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027193555709.png)

**Example 2ï¼š**

![image-20231027193646370](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027193646370.png)

**è‡ªå·±ä¸Šä¼ å›¾åƒè¿›è¡Œå›¾åƒå¢å¼ºæ ‡è®°ï¼š**

æ•ˆæœçœ‹èµ·æ¥ç¡®å®å¾ˆå¥½ï¼Œä¹‹åæˆ‘ä»¬è¿›è¡Œæ¼”ç¤ºçš„å›¾åƒå¯ä»¥å…ˆä½¿ç”¨SoMè¿›è¡Œå›¾åƒä¿¡æ¯æ ‡è®°å¢å¼ºï¼Œä¹‹åå†è¾“å…¥åˆ°è§†è§‰æ¨¡å‹ä¸­ï¼Œè¿™æ ·å¯ä»¥å¤§å¤§æé«˜æ¨¡å‹æ•ˆæœã€‚

![image-20231027193943396](C:\Users\KevinGeorge\AppData\Roaming\Typora\typora-user-images\image-20231027193943396.png)



